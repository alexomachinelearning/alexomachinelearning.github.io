---
layout: post
title: "New Terms and Concepts for Further Study I"
date:   2019-01-10 12:00:00 -0800

---

The video ["Gradient descent, how neural networks learn"](https://www.youtube.com/watch?v=IHZwWFHWa-w) by 3Blue1Brown is a good introduction to a more granular view of neural networks. The video focuses on the example of a multilayer perceptron neural network being trained, with supervised learning, to recognize (predict) the value of a handwritten number in an image. The video's main focus areas are gradient descent and its relationship to the training process, namely what the hidden layers in the multilayer perceptron are doing mathematically, and how the network arrives at its prediction in the output layer.

(For even greater detail on these concepts, see Michael Nielsen's free online book [_Neural Networks and Deep Learning_](http://neuralnetworksanddeeplearning.com/), used as inspiration for the video.)

Below I list terms and concepts for further study.

## Terms and Concepts

NETWORK STRUCTURE

* hidden layer(s)
* input layer
* neurons
* output layer


TRAINING PROCESS
* activations
* biases
* binary activation
* continuous activation
* generalization
* initialization
* learning
* parameters
* training data
* weights


MATH
* average cost
* backpropagation
* column vector
* cost function
* global minimum
* hyper-dimensional spaces
* length of the gradient vector
* local minimum
* minima of a function
* negative gradient
* slope of the function
* step size
* ReLU function
* sigmoid function

---

I wonder if, in addition to learning about the above, studying how to read topographic maps could help visualize the math behind gradient descent, the core mathematical process of neural network training.
