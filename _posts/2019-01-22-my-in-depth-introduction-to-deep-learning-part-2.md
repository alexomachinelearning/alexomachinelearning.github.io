---
layout: post
title: "My In-Depth Introduction to Deep Learning Basics (Part 2)"
date:   2019-01-22 12:00:00 -0800

---
Yesterday in ["My In-Depth Introduction to Deep Learning Basics (Part 1)"](https://ahumanlearningmachinelearning.com/2019/01/21/my-in-depth-introduction-to-deep-learning-part-1/), I went through the first 40% of the slides in the first lecture of Lex Fridman's MIT Deep Learning course _6.S094: Deep Learning for Self-Driving Cars_. Those slides can be found [here](https://www.dropbox.com/s/c0g3sc1shi63x3q/deep_learning_basics.pdf?dl=0#). While doing so, I made comments related to questions, ideas, connections, and learnings from the last few weeks connected to the slides. Today's post continues where I left off: slide 23 and beyond.

## The Slide into Deep Learning Continues

- **S26**: the _Testing Stage_ diagrammed here seems to correspond with part 6 ("# make predictions") of the TensorFlow code shown on Slide 6
- **S28**: In ["A Simple Exercise to Understand the Prediction-Making Conducted by Machines"](https://ahumanlearningmachinelearning.com/2019/01/19/a-simple-exercise-to-understand-the-prediction-making-conducted-by-machines/), I walked through an example of regression using the point-slope formula many of us learned in grade school to calculate the y-intercept or slope of a line, of which the top half of this slide reminds me. It's interesting to see that for some questions, such as the Cold/Hot question asked in the bottom half here, each of two binary answers includes a linear range of values within which that particular answer classifies as correct. So what appears as a Cold/Hot question includes the equivalent of "if a &le; x &le; b, then x = Cold, else if b &#60; x &le; c, x = Hot"
- **S32**: Hmm. I hadn't thought about this. There's something I remember from my neuroscience classes, about how action potentials propagate only once a certain threshold has been crossed. This is the so-called "all-or-nothing" response (which I see is now referred to as the [all-or-none law](https://en.wikipedia.org/wiki/All-or-none_law)). Here, per ["Threshold potential"](https://en.wikipedia.org/wiki/Threshold_potential): "In neuroscience, the threshold potential is the critical level to which a membrane potential must be depolarized to initiate an action potential." This is a different context, but consider what slide 32 shows as the mathematics of an artificial neuron: &#8721;w<sub>i</sub>x<sub>i</sub> + b. The b is the bias, and from what I've learned, it serves as a sort of threshold potential too
- **S33**: Very interesting slide on the differences between biological neurons in humans and artificial neurons in machine learning software. To the point that we're only just scratching the surface of what we know about human learning, the slide specifies that "[Artificial neural networks] use gradient descent for learning. We don't know what human brains use". Separately, on power consumption, it's interesting to consider that a good portion of brain activity goes to visual processing in the occipital lobe, such is the importance of vision in humans. I wonder if any machine learning systems out there that incorporate artificial neural networks of many kinds (e.g., autonomous vehicles) also replicate this in the design of their power loads, with vision-oriented ANNs receiving some disproportionate energy priority. I'll also have to look into if there's any research out there on the computer equivalent of ATP, from an energy use and computation standpoint
- **S36**: This is coolâ€”I didn't know that _TPUs_ are simply ASICs (application-specific integrated circuits). On an unrelated note, in Bitcoin mining, ASICs long ago supplanted GPUs as the minimum necessary hardware for participation as a miner in the autonomous timestamp network the Proof of Work system is designed to produce (as part of the Bitcoin blockchain's solution architecture). See [Bitcoin White Paper, section 4: Proof-of-Work](https://bitcoin.org/bitcoin.pdf)

I'll end there for now, as the next set of slides get into the math-heavy concepts of activation functions, loss functions, and the process by which neural networks utilize backpropagation to "update the weights and biases to decrease loss function" (S40).

Tomorrow, the fun resumes.
