---
layout: post
title: "On My Priority List: Neural Network Zoo and Prequel"
date:   2019-01-29 12:00:00 -0800

---

Over the last few days, I've been exposed to many different neural network types in conversations and resources I've encountered. This reminded me that one of the first resources I learned about was [The Asimov Institute's](http://www.asimovinstitute.org/) excellent two-part introduction to neural network architecture: [The Neural Network Zoo Prequel: Cells and Layers](http://www.asimovinstitute.org/neural-network-zoo-prequel-cells-layers/), and [The Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/).

The Prequel covers "cells" (neurons), including the inputs, outputs, sums, biases, gates, and activation functions that can be associated with a given neuron. Recurrent weights, different types of cells (from basic to LSTM) and layers of connections are covered.

The Neural Network Zoo depicts and describes 27 neural network architectures, starting with the perceptron and ending with the neural Turing Machine (NTM). Three types of input cells, three types of hidden cells, 2 types of output cells, three types of memory cells (including the recurrent cell), and a kernel and convolution / pool are all explored.

Together, these two resources make required reading for beginners like me who have achieved reasonable starting familiarity with machine learning concepts and are ready to map technical choices in neural network design to the specific problem sets those networks are optimized to solve.

The two articles are a combined ~12,000 words (about an hour of reading). How long it will take to digest it all is another matter.
