---
layout: post
title: "The Mainframe Days of Machine Learning"
date:   2019-01-07 12:00:00 -0800
redirect_from:
  - /2019-01-07-the-mainframe-days-of-machine-learning.html
---

Tracy Kidder's 1981 Pulitzer-winner, [_The Soul of a New Machine_](https://www.amazon.com/Soul-New-Machine-Tracy-Kidder/dp/0316491977), tells the story of Data General, a computer company that in the 1970s built popular _minicomputers_, which were successors to mainframes but precursors-in-time to the personal computers that made companies like Microsoft and Apple behemoths in the late 1980s and 1990s. In the two decades between when the first neural networks were built (1959) and when a little company called Microsoft moved from Albuquerque, NM to Bellevue, WA (1979), minicomputers were the rage, and companies like Data General made them that way. (See ["A Little on Early History in Machine Learning"](https://ahumanlearningmachinelearning.com/2019/01/05/a-little-on-early-history-in-machine-learning.html); ["Minicomputer"](https://en.wikipedia.org/wiki/Minicomputer); ["Microsoft"](https://en.wikipedia.org/wiki/Microsoft)).

In the first chapter of Kidder's excellent book, the following is provided for context (pp.11-12):

> "In the early days, computers [mainframes] inspired widespread awe and the popular press dubbed them giant brains. [...] But computers were relatively scarce, and they were large and very expensive. Typically, one big machine served an entire organization. [...] Scientists and engineers, it seems, were the first to express a desire for a relatively inexpensive computer that they could operate themselves. The result was a machine called a minicomputer."

Two things there fascinate: one, that the mainframe was once considered a "giant brain", because a single iPhone today has far more computing power [than did some mainframes of that era](https://www.businessinsider.com/ibm-1970-mainframe-specs-are-ridiculous-today-2014-5), yet a single iPhone has far less computational power than a human brain. And two, the enduring role of scientists and engineers in driving market forces in sometimes unseen ways.

## Our Mainframe Days

Today, I met with a data engineer, Bilal K., who is also an enthusiast of deep learning. He had suggested I look into Fast.ai's free online course ["Introduction to Machine Learning for Coders"](https://www.fast.ai./), and we grabbed coffee. Afterwards, I wrote [a LinkedIn post](https://www.linkedin.com/feed/update/urn:li:activity:6488237468326989824) sharing what I had learned. Here's the relevant snippet:

>Bilal mentioned that enterprise expertise in ML can currently be found in companies like Facebook, Google, Amazon, Apple, Tencent, Baidu, and Alibaba but that the potential of deep learning in commercial and consumer applications is only just starting to be known. That as areas like transfer learning, generative adversarial networks, and natural language processing continue to advance, and as it becomes easier for devs to build and deploy more advanced ML applications, the democratization of ML will swing into high gear. It made me consider if we’re still in “the mainframe days” of machine learning.

From the little I've learned on my new learning journey about this technology, I can already tell that the technology is consequential. But we are still in the earliest days of commercialized machine learning. One thing I've come to know from my time working in the decentralized blockchain space is that it takes time for three things to happen:

* for new technology to transform markets
* for the first-generation impact of new technologies, however big that impact may be in absolute terms, to be placed in the relative context it deserves
* and—to channel Kidder—for new machines and their technologies to develop soul

Perhaps someone will write a Pulitzer-winning book someday about the mainframe days of the very subject we are all currently learning about.
