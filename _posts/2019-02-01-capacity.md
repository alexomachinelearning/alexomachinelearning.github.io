---
layout: post
title: "Capacity"
date:   2019-02-01 12:00:00 -0800

---
I read a page, on **capacity**, of Ian Goodfellow et al's 2016 book [_Deep Learning_](https://www.deeplearningbook.org/) today.

Capacity is a measure of how well a learning model, once trained, will be able to handle test data. As best I can tell:

If the capacity is too low, _underfitting_ happens, and the model ends up not working on data that isn't identical to the training data.

If the capacity is too high, then _overfitting_ happens instead, and the mathematical relationships the learning model learns during training end up being applied too judiciously to subsequent data, even if the best way to mathematically describe some of that data is not actually with that same math:

> "Machine learning algorithms will generally perform best when their
capacity is appropriate for the true complexity of the task they need to perform and the amount of training data they are provided with. Models with high capacity can solve complex tasks, but when their capacity is higher than needed to solve the present task, they may overfit."

This made me reflect on how and when human learning prevents underfitting or overfitting, and the cognitive problems that emerge when our ability to avoid overfitting fails us.
